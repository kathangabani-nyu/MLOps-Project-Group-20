#!/usr/bin/env python

"""
train_bart_ray_mlflow.py
------------------------
Fine-tunes facebook/bart-large-cnn on CNN-DailyMail 3.0.0 with:
 * MLflow tracking + model registry
 * Optional LoRA PEFT
 * Gradient accumulation & bf16
 * Ray logic commented for now
"""

import os
from datetime import datetime
import mlflow
from datasets import load_dataset
import torch
from transformers import (
    BartTokenizerFast, BartForConditionalGeneration,
    Seq2SeqTrainingArguments, Seq2SeqTrainer,
    DataCollatorForSeq2Seq, set_seed
)
from peft import get_peft_model, LoraConfig, TaskType

# ------------------------------------------------
# Force MLflow to use the newly initialized server
# ------------------------------------------------
# Unconditionally set the correct MLflow tracking URI
mlflow.set_tracking_uri("http://129.114.25.36:8000")

########################################
# CONFIGURATION
########################################

gpus = 2
batch = 2
grad_accum = 8
epochs = 3
precision = "bf16"
strategy = "ddp"
lora_r = 16
output_dir = os.getenv(
    "OUTPUT_DIR",
    "/mnt/object/transcept-summarization/bart_output"
)
resume_from = None
seed = 1234
model_id = "facebook/bart-large-cnn"

########################################
# MLflow SETUP
########################################

set_seed(seed)
mlflow.set_experiment("transcept-summarization")
run = mlflow.start_run(
    run_name=f"bart_notebook_{datetime.now():%Y%m%d_%H%M%S}"
)
run_id = run.info.run_id

########################################
# MODEL & TOKENIZER
########################################

tokenizer = BartTokenizerFast.from_pretrained(model_id)
model = BartForConditionalGeneration.from_pretrained(model_id)
if lora_r > 0:
    lora_cfg = LoraConfig(
        r=lora_r, lora_alpha=32, lora_dropout=0.05,
        bias="none", task_type=TaskType.SEQ_2_SEQ_LM
    )
    model = get_peft_model(model, lora_cfg)

########################################
# DATASET PREP
########################################

raw_ds = load_dataset("cnn_dailymail", "3.0.0")
train_ds = raw_ds["train"].shuffle(seed=42).select(range(500))
eval_ds = raw_ds["validation"].shuffle(seed=42).select(range(100))

def preprocess(batch):
    inputs = tokenizer(
        batch["article"], truncation=True, padding="max_length",
        max_length=1024
    )
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            batch["highlights"], truncation=True, padding="max_length",
            max_length=128
        )
    inputs["labels"] = labels["input_ids"]
    return inputs

train_ds = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)
eval_ds = eval_ds.map(preprocess, batched=True, remove_columns=eval_ds.column_names)

########################################
# TRAINING ARGUMENTS
########################################

bf16 = precision == "bf16" and torch.cuda.is_bf16_supported()
args = Seq2SeqTrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=batch,
    per_device_eval_batch_size=batch,
    gradient_accumulation_steps=grad_accum,
    learning_rate=3e-5,
    num_train_epochs=epochs,
    logging_strategy="epoch",
    eval_strategy="epoch",
    save_strategy="epoch",
    predict_with_generate=True,
    bf16=bf16,
    fp16=False,
    dataloader_drop_last=True,
    seed=seed,
    ddp_find_unused_parameters=False if gpus > 1 else None,
    report_to="none"
)
collator = DataCollatorForSeq2Seq(tokenizer, model)

########################################
# TRAIN
########################################

trainer = Seq2SeqTrainer(
    model=model,
    args=args,
    train_dataset=train_ds,
    eval_dataset=eval_ds,
    data_collator=collator,
)
if resume_from:
    trainer.train(resume_from_checkpoint=resume_from)
else:
    trainer.train()

########################################
# EVALUATE & LOG
########################################

metrics = trainer.evaluate(max_length=128, num_beams=4)
print(metrics)
mlflow.log_metrics(metrics)

ckpt_path = os.path.join(output_dir, "final")
trainer.save_model(ckpt_path)
tokenizer.save_pretrained(ckpt_path)
mlflow.pytorch.log_model(trainer.model, "model")
mlflow.end_run()

########################################
# RAY PART (commented)
########################################

# import ray
# from ray import train
# from ray.train import ScalingConfig, Checkpoint
# from ray.train.torch import TorchTrainer
#
# def ray_train_loop(config):
#     # define your train loop here
#     pass
#
# ray.init()
# scaling = ScalingConfig(num_workers=gpus, use_gpu=True, resources_per_worker={"GPU": 1})
#
# trainer = TorchTrainer(
#     train_loop_per_worker=ray_train_loop,
#     scaling_config=scaling,
#     run_config=train.RunConfig(storage_path=output_dir),
#     train_loop_config={}
# )
# result = trainer.fit()
# print("Best checkpoint:", result.checkpoint.uri)
# ray.shutdown()
